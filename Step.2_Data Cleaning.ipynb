{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_93iD1Qdrnd0"
   },
   "source": [
    "# Step.2 Data Cleaning and Manipulation:\n",
    "Data cleaning was performed to identify and address errors, handle missing values, and ensure consistency across the dataset, preparing it for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzlMua7GrtXK"
   },
   "source": [
    "**2.1 Import neccesary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kXln8_tTriYD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#for checking misspells\n",
    "from fuzzywuzzy import fuzz\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I59TlKd5rmoJ"
   },
   "source": [
    "**2.2 Read the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8owlzaPosQTz"
   },
   "outputs": [],
   "source": [
    "# Read files into DataFrames\n",
    "\n",
    "df_geolocation = pd.read_csv(\"olist_geolocation_dataset_2.0.csv\") #Use the processed dataset: olist_geolocation_dataset_2.0.\n",
    "df_orders = pd.read_csv(\"olist_orders_dataset.csv\")\n",
    "df_payments = pd.read_csv(\"olist_order_payments_dataset.csv\")\n",
    "df_reviews = pd.read_csv(\"olist_order_reviews_dataset.csv\")\n",
    "df_products = pd.read_csv(\"olist_products_dataset.csv\")\n",
    "df_product_translation = pd.read_csv(\"product_category_name_translation.csv\")\n",
    "df_customers = pd.read_csv(\"olist_customers_dataset.csv\")\n",
    "df_sellers = pd.read_csv(\"olist_sellers_dataset.csv\")\n",
    "df_items = pd.read_csv(\"olist_order_items_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Process all datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7255bq_Kyg6t"
   },
   "source": [
    "**2.3.1 Orders Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p8rPKQAjyjx0"
   },
   "outputs": [],
   "source": [
    "#%%# List of date columns\n",
    "date_columns = [\n",
    "    'order_purchase_timestamp',\n",
    "    'order_approved_at',\n",
    "    'order_delivered_carrier_date',\n",
    "    'order_delivered_customer_date',\n",
    "    'order_estimated_delivery_date'\n",
    "]\n",
    "\n",
    "# Check the NA values and duplicates\n",
    "df_orders.isna().sum()\n",
    "df_orders['order_id'].duplicated().sum()\n",
    "\n",
    "# Extracts just the date part\n",
    "df_orders[date_columns] = df_orders[date_columns].apply(pd.to_datetime, errors='coerce')\n",
    "df_orders = df_orders.dropna(subset=date_columns)\n",
    "\n",
    "# Check the integrity of the date columns based on the new conditions\n",
    "integrity_check = (\n",
    "    (df_orders['order_purchase_timestamp'] <= df_orders['order_approved_at']) &  # order_purchase_timestamp should be earlier than order_approved_at\n",
    "    (df_orders['order_approved_at'] <= df_orders['order_delivered_carrier_date']) &  # order_approved_at should be earlier than order_delivered_carrier_date\n",
    "    (df_orders['order_delivered_carrier_date'] <= df_orders['order_delivered_customer_date'])  # order_delivered_carrier_date should be earlier than order_delivered_customer_date\n",
    ")\n",
    "\n",
    "# Remove rows that violate the integrity checks\n",
    "df_orders = df_orders[integrity_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7BuTNdLzJdb"
   },
   "source": [
    "**2.3.2 Payments Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r1F481eFzPun"
   },
   "outputs": [],
   "source": [
    "# Checking for NA values: no nan value\n",
    "df_payments.isna().sum()\n",
    "\n",
    "# Checking for duplicates: 4446 duplicates\n",
    "df_payments['order_id'].duplicated().sum()\n",
    "\n",
    "# Displaying duplicate rows:\n",
    "duplicates = df_payments[df_payments.duplicated(subset='order_id', keep=False)].sort_values(by='order_id')\n",
    "#The duplicate order_id in the payments are due to multiple payment methods for one order, such as by credit card and by voucher, which would result in two rows.\n",
    "\n",
    "# Select only column 'order_id' , 'payment_value' , 'payment_type'\n",
    "df_payments = df_payments[['order_id','payment_type', 'payment_value']]\n",
    "\n",
    "# Based on the above discovery, combining the same orders into the same rows:\n",
    "df_payments = df_payments.groupby('order_id').agg(\n",
    "    payment_value_sum=('payment_value', 'sum'),\n",
    "    payment_types_combined=('payment_type', lambda x: ','.join(sorted(set(x))))\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeozOnn7zUaw"
   },
   "source": [
    "**2.3.3 Reviews Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mtz_hZ-1zg4i"
   },
   "outputs": [],
   "source": [
    "# Select only column 'review_id', 'order_id' , 'review_score' , 'review_comment_message'\n",
    "df_reviews['is_review'] = df_reviews['review_comment_message'].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "df_reviews = df_reviews[['review_id', 'order_id', 'review_score','is_review']]\n",
    "\n",
    "# Some review_id may correspond to multiple orders.\n",
    "duplicates_reviews = df_reviews[df_reviews['review_id'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.4 Products Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge translation datasets to translate product categories into English.\n",
    "df_products = df_products.merge(df_product_translation, on='product_category_name', how='left')\n",
    "\n",
    "# Select only column 'product_id', 'product_category_name' , 'product_description_lenght' , 'product_photos_qty','product_category_name_english'\n",
    "df_products = df_products[['product_id', 'product_category_name','product_photos_qty','product_category_name_english']]\n",
    "\n",
    "#Drop all null values\n",
    "df_products.isnull().sum()\n",
    "df_products = df_products.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4K0rvMczkCF"
   },
   "source": [
    "**2.3.5 Customers Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jMutXQ3LzqDE"
   },
   "outputs": [],
   "source": [
    "# Check N/A & duplicated value\n",
    "df_customers.isnull().sum()\n",
    "df_customers['customer_id'].duplicated().sum()\n",
    "df_customers['customer_unique_id'].duplicated().sum() # that is acceptable as each of the customer may make several orders(customer_id in this dataset)\n",
    "\n",
    "# Joined with geolocation dataset to know each customer latitude and longitude.\n",
    "df_customers = df_customers.merge(df_geolocation,\n",
    "                                  left_on='customer_zip_code_prefix',right_on = 'geolocation_zip_code_prefix',how = 'left')\n",
    "df_customers  = df_customers.drop(columns=['geolocation_city','normalized_city'])\n",
    "\n",
    "df_customers['is_same_state'] = df_customers['customer_state'] == df_customers['geolocation_state']\n",
    "\n",
    "# Check that the geographic information in the customer dataset is consistent with the geolocation data set.\n",
    "df_customers = df_customers[df_customers['is_same_state']!=False]\n",
    "\n",
    "# Change the name of the columns to avoid confusing columns after combination of data\n",
    "df_customers = df_customers.rename(columns={'geolocation_lat': 'customer_lat_n', 'geolocation_lng': 'customer_lng_n'})\n",
    "df_customers = df_customers.rename(columns={'geolocation_state': 'customer_state_n', 'corrected_city': 'customer_city_n'})\n",
    "\n",
    "# Select only column 'customer_id', 'customer_unique_id' , 'customer_zip_code_prefix' , 'customer_state_n','customer_city_n','customer_lat_n','customer_lng_n'\n",
    "df_customers = df_customers[['customer_id', 'customer_unique_id', 'customer_zip_code_prefix','customer_state_n','customer_city_n','customer_lat_n','customer_lng_n']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb7sQbKvzshH"
   },
   "source": [
    "**2.3.6 Sellers Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j14e6l54zvsk"
   },
   "outputs": [],
   "source": [
    "df_sellers.isnull().sum()\n",
    "df_sellers['seller_id'].duplicated().sum()\n",
    "\n",
    "## Joined with geolocation dataset to know each seller latitude and longitude.\n",
    "df_sellers = df_sellers.merge(df_geolocation,left_on='seller_zip_code_prefix',right_on = 'geolocation_zip_code_prefix',how = 'left' )\n",
    "df_sellers  = df_sellers.drop(columns=['geolocation_city','normalized_city'])\n",
    "\n",
    "# Check that the geographic information in the seller dataset is consistent with the geolocation data set.\n",
    "df_sellers['is_same_state'] = df_sellers['seller_state'] == df_sellers['geolocation_state']\n",
    "df_sellers['is_same_city'] = df_sellers['seller_city'] == df_sellers['corrected_city']\n",
    "different_state_or_city = df_sellers[(df_sellers['is_same_state'] == False) | (df_sellers['is_same_city'] == False)]\n",
    "\n",
    "df_sellers = df_sellers[(df_sellers['is_same_state'] != False) & (df_sellers['is_same_city'] != False)]\n",
    "df_sellers = df_sellers.dropna()\n",
    "\n",
    "# Change the name of the columns to avoid confusing columns after combination of data\n",
    "df_sellers = df_sellers.rename(columns={'geolocation_lat': 'seller_lat_n', 'geolocation_lng': 'seller_lng_n'})\n",
    "df_sellers = df_sellers.rename(columns={'geolocation_state': 'seller_state_n', 'corrected_city': 'seller_city_n'})\n",
    "\n",
    "df_sellers = df_sellers[['seller_id', 'seller_zip_code_prefix','seller_city_n','seller_state_n','seller_lat_n','seller_lng_n']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb3NMC-c1KO9"
   },
   "source": [
    "**2.3.7 Order Item Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CrDMc8wV1RMl"
   },
   "outputs": [],
   "source": [
    "# Find and drop duplicates in order items dataset\n",
    "duplicate_ORDER_ITEMS = df_items[df_items.duplicated(subset='order_id', keep=False)]\n",
    "df_items = df_items.drop_duplicates(subset='order_id', keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4UHh39E1iEW"
   },
   "source": [
    "# Merge:\n",
    "Merge the data and store in a new csv file which will be used for feature modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vDVcoQ3l1m7I"
   },
   "outputs": [],
   "source": [
    "#%% Merge data\n",
    "\n",
    "df = df_reviews.merge(df_orders, on='order_id', how='left', indicator=True) # the orders without review won't included\n",
    "df = df.merge(df_customers, on='customer_id', how='left')\n",
    "df = df.merge(df_items, on='order_id', how='left')\n",
    "df = df.merge(df_payments, on='order_id', how='left')\n",
    "df = df.merge(df_products, on='product_id', how='left')\n",
    "df = df.merge(df_sellers, on='seller_id', how='left')\n",
    "\n",
    "#DROP unnecessary columns\n",
    "df.drop(columns=[\n",
    "    'customer_id',\n",
    "    '_merge',\n",
    "    'customer_zip_code_prefix',\n",
    "    'order_item_id',\n",
    "    'product_category_name',\n",
    "    'seller_zip_code_prefix'],\n",
    "    inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "df = df.rename(columns={'customer_state_n': 'customer_state',\n",
    "                        'customer_city_n': 'customer_city',\n",
    "                        'customer_lat_n':'customer_lat',\n",
    "                        'customer_lng_n':'customer_lng',\n",
    "                        'payment_value_sum':'payment_value',\n",
    "                        'payment_types_combined':'payment_types',\n",
    "                        'product_category_name_english':'product_category_name',\n",
    "                        'seller_city_n':'seller_city',\n",
    "                        'seller_state_n':'seller_state',\n",
    "                        'seller_lat_n':'seller_lat',\n",
    "                        'seller_lng_n':'seller_lng'})\n",
    "\n",
    "\n",
    "duplicates = df[df.duplicated()] #0\n",
    "df_na_rows = df[df.isna().any(axis=1)]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datasetfinal2.0.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
